{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "model_name = 'DeepPavlov/bert-base-multilingual-cased-sentence'\n",
    "# model_name = 'DeepPavlov/distilrubert-small-cased-conversational'\n",
    "# model_name = 'DeepPavlov/distilrubert-tiny-cased-conversational'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_header(note):\n",
    "    if '## ' in note:\n",
    "        first_header_prefix = '## '\n",
    "        if '### ' in note:\n",
    "            first_header_prefix = '### '\n",
    "\n",
    "        first_header = note.split(first_header_prefix)[1].split('\\n')[0]\n",
    "    else:\n",
    "        first_header = ''\n",
    "\n",
    "    return first_header\n",
    "\n",
    "def clean(note):\n",
    "    # remove zero-links\n",
    "    note = re.sub(r'\\[.*\\]', '', note)\n",
    "\n",
    "    # remove tags and headers\n",
    "    note = re.sub(r'\\#.*\\n', '', note)\n",
    "\n",
    "    # remove \\n\n",
    "    note = re.sub('\\n', ' ', note)\n",
    "\n",
    "    # remove lines\n",
    "    note = re.sub('---', ' ', note)\n",
    "\n",
    "    # remove **\n",
    "    note = re.sub('\\*', '', note)\n",
    "    \n",
    "    return note\n",
    "\n",
    "def num_letters(note):\n",
    "    return len(re.sub(r'[^а-яА-Яa-zA-Z]', '', note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_thr = 100\n",
    "device = torch.device(0)\n",
    "# device = 'cpu'\n",
    "encode_kwargs = {'truncation': True, 'padding': 'max_length', 'pad_to_multiple_of': 1, 'max_length':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(text):\n",
    "    if ('abstract' in text.lower()) and ('ntroduction' in text.lower()):\n",
    "        start_pos = text.lower().index('abstract') + len('abstract')\n",
    "        end_pos = text.lower().index('ntroduction') - 1\n",
    "        abstract = text[start_pos: end_pos]\n",
    "    else:\n",
    "        abstract = text\n",
    "\n",
    "    abstract = re.sub('-\\n', '', abstract)\n",
    "    abstract = re.sub('\\n', ' ', abstract)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martins et al. - 2022 - $infty$-former Infinite Memory Transformer.pdf',\n",
       " '.zotero-ft-cache',\n",
       " '.zotero-ft-info']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/home/booydar/Documents/Sync/zotero/storage/'\n",
    "path = db_path\n",
    "def parse_storage(path):\n",
    "\n",
    "    db_df = pd.DataFrame()\n",
    "    path, folders, files = next(os.walk(path))\n",
    "    for f in folders:\n",
    "        f_path = os.path.join(path, f)\n",
    "        path_, folders_, files_ = next(os.walk(f_path))\n",
    "        pdfs = [n for n in files_ if '.pdf' in n]\n",
    "        if not pdfs:\n",
    "            continue\n",
    "        paper = pdfs[0]\n",
    "        paper_path = os.path.join(f_path, paper)\n",
    "        reader = PdfReader(paper_path)\n",
    "        page = reader.pages[0]\n",
    "        first_page = page.extract_text()\n",
    "        abstract = get_abstract(first_page)\n",
    "\n",
    "        paper_dict = {'path': paper_path,\n",
    "                    'paper': paper,\n",
    "                    'name': re.sub(r'.*-* - ', '', paper)[:-4], \n",
    "                    'abstract': abstract}\n",
    "\n",
    "        db_df = pd.concat([db_df, pd.DataFrame(paper_dict, index=[0])])\n",
    "        \n",
    "    return db_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df = parse_storage(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>paper</th>\n",
       "      <th>name</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/SF...</td>\n",
       "      <td>Jaegle et al. - 2022 - Perceiver IO A General ...</td>\n",
       "      <td>Perceiver IO A General Architecture for Structure</td>\n",
       "      <td>A central goal of machine learning is the dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/KS...</td>\n",
       "      <td>Liška et al. - 2018 - Memorize or generalize S...</td>\n",
       "      <td>Memorize or generalize Searching for a compositio</td>\n",
       "      <td>Neural networks are very powerful learning sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/TQ...</td>\n",
       "      <td>Yao et al. - 2019 - KG-BERT BERT for Knowledge...</td>\n",
       "      <td>KG-BERT BERT for Knowledge Graph Completion</td>\n",
       "      <td>Knowledge graphs are important resources for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/EV...</td>\n",
       "      <td>Lample et al. - 2019 - Large Memory Layers wit...</td>\n",
       "      <td>Large Memory Layers with Product Keys</td>\n",
       "      <td>This paper introduces a structured memory whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/EH...</td>\n",
       "      <td>Voita et al. - 2019 - When a Good Translation ...</td>\n",
       "      <td>When a Good Translation is Wrong in Context Conte</td>\n",
       "      <td>Though machine translation errors caused by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/6P...</td>\n",
       "      <td>Sun et al. - 2021 - Do Long-Range Language Mod...</td>\n",
       "      <td>Do Long-Range Language Models Actually Use Lon...</td>\n",
       "      <td>Language models are generally trained on shor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/3G...</td>\n",
       "      <td>Hutchins et al. - 2022 - Block-Recurrent Trans...</td>\n",
       "      <td>Block-Recurrent Transformers</td>\n",
       "      <td>We introduce the Block-Recurrent Transformer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/MF...</td>\n",
       "      <td>Zhang et al. - 2020 - PEGASUS Pre-training wit...</td>\n",
       "      <td>PEGASUS Pre-training with Extracted Gap-sentences</td>\n",
       "      <td>ive Summarization Jingqing Zhang* 1Yao Zhao* 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/UL...</td>\n",
       "      <td>Ba et al. - 2016 - Using Fast Weights to Atten...</td>\n",
       "      <td>Using Fast Weights to Attend to the Recent Past</td>\n",
       "      <td>Until recently, research on artiﬁcial neural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/booydar/Documents/Sync/zotero/storage/FY...</td>\n",
       "      <td>Martins et al. - 2022 - $infty$-former Infinit...</td>\n",
       "      <td>$infty$-former Infinite Memory Transformer</td>\n",
       "      <td>Transformers are unable to model long-term me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  \\\n",
       "0   /home/booydar/Documents/Sync/zotero/storage/SF...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/KS...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/TQ...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/EV...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/EH...   \n",
       "..                                                ...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/6P...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/3G...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/MF...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/UL...   \n",
       "0   /home/booydar/Documents/Sync/zotero/storage/FY...   \n",
       "\n",
       "                                                paper  \\\n",
       "0   Jaegle et al. - 2022 - Perceiver IO A General ...   \n",
       "0   Liška et al. - 2018 - Memorize or generalize S...   \n",
       "0   Yao et al. - 2019 - KG-BERT BERT for Knowledge...   \n",
       "0   Lample et al. - 2019 - Large Memory Layers wit...   \n",
       "0   Voita et al. - 2019 - When a Good Translation ...   \n",
       "..                                                ...   \n",
       "0   Sun et al. - 2021 - Do Long-Range Language Mod...   \n",
       "0   Hutchins et al. - 2022 - Block-Recurrent Trans...   \n",
       "0   Zhang et al. - 2020 - PEGASUS Pre-training wit...   \n",
       "0   Ba et al. - 2016 - Using Fast Weights to Atten...   \n",
       "0   Martins et al. - 2022 - $infty$-former Infinit...   \n",
       "\n",
       "                                                 name  \\\n",
       "0   Perceiver IO A General Architecture for Structure   \n",
       "0   Memorize or generalize Searching for a compositio   \n",
       "0         KG-BERT BERT for Knowledge Graph Completion   \n",
       "0               Large Memory Layers with Product Keys   \n",
       "0   When a Good Translation is Wrong in Context Conte   \n",
       "..                                                ...   \n",
       "0   Do Long-Range Language Models Actually Use Lon...   \n",
       "0                        Block-Recurrent Transformers   \n",
       "0   PEGASUS Pre-training with Extracted Gap-sentences   \n",
       "0     Using Fast Weights to Attend to the Recent Past   \n",
       "0          $infty$-former Infinite Memory Transformer   \n",
       "\n",
       "                                             abstract  \n",
       "0    A central goal of machine learning is the dev...  \n",
       "0    Neural networks are very powerful learning sy...  \n",
       "0    Knowledge graphs are important resources for ...  \n",
       "0    This paper introduces a structured memory whi...  \n",
       "0    Though machine translation errors caused by t...  \n",
       "..                                                ...  \n",
       "0    Language models are generally trained on shor...  \n",
       "0    We introduce the Block-Recurrent Transformer,...  \n",
       "0   ive Summarization Jingqing Zhang* 1Yao Zhao* 2...  \n",
       "0    Until recently, research on artiﬁcial neural ...  \n",
       "0    Transformers are unable to model long-term me...  \n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (790908752.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [108]\u001b[0;36m\u001b[0m\n\u001b[0;31m    note += f\"##{row['name']}\\n\\\"\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "out_path = '/home/booydar/Documents/Sync/scientific_graph'\n",
    "\n",
    "for i, row in db_df.iterrows():\n",
    "    \n",
    "    note = ''\n",
    "    note += '#paper\\n---\\n\\n'\n",
    "    note += f\"##{row['name']}\\n\\n\"\n",
    "    note += f'{row.abstract}\\n\\n---\\n'\n",
    "\n",
    "    note_name = row.paper[:-4] + '.md'\n",
    "    op = os.path.join(out_path, note_name)\n",
    "\n",
    "    with open(op, 'w') as f:\n",
    "        f.write(note)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('cudaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13243d513f3ec5f72a4e7429086ed10f2270caad452cf1f965679cad914ac74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
