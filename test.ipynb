{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "model_name = 'DeepPavlov/bert-base-multilingual-cased-sentence'\n",
    "# model_name = 'DeepPavlov/distilrubert-small-cased-conversational'\n",
    "# model_name = 'DeepPavlov/distilrubert-tiny-cased-conversational'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_header(note):\n",
    "    if '## ' in note:\n",
    "        first_header_prefix = '## '\n",
    "        if '### ' in note:\n",
    "            first_header_prefix = '### '\n",
    "\n",
    "        first_header = note.split(first_header_prefix)[1].split('\\n')[0]\n",
    "    else:\n",
    "        first_header = ''\n",
    "\n",
    "    return first_header\n",
    "\n",
    "def clean(note):\n",
    "    # remove zero-links\n",
    "    note = re.sub(r'\\[.*\\]', '', note)\n",
    "\n",
    "    # remove tags and headers\n",
    "    note = re.sub(r'\\#.*\\n', '', note)\n",
    "\n",
    "    # remove \\n\n",
    "    note = re.sub('\\n', ' ', note)\n",
    "\n",
    "    # remove lines\n",
    "    note = re.sub('---', ' ', note)\n",
    "\n",
    "    # remove **\n",
    "    note = re.sub('\\*', '', note)\n",
    "    \n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_thr = 20\n",
    "# device = 'cuda'\n",
    "device = 'cpu'\n",
    "encode_kwargs = {'truncation': True, 'padding': 'max_length', 'pad_to_multiple_of': 1, 'max_length':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/home/booydar/Documents/Sync/obsidian-db/'\n",
    "\n",
    "\n",
    "def parse_folder(path):\n",
    "    # print(f'Parsing {path}')\n",
    "\n",
    "    path, folders, files = next(os.walk(path))\n",
    "\n",
    "    db_df = pd.DataFrame()\n",
    "    if len(folders) > 0:\n",
    "        for f in folders:\n",
    "            folder_path = os.path.join(path, f)\n",
    "            f_res_df = parse_folder(folder_path)\n",
    "            db_df = pd.concat([db_df, f_res_df])\n",
    "\n",
    "    for fn in files:\n",
    "        if '.md' not in fn:\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(path, fn)\n",
    "        with open(filepath, 'r') as f:\n",
    "            note = f.read()\n",
    "\n",
    "        if len(note) < length_thr:\n",
    "            continue\n",
    "\n",
    "        header = get_first_header(note)\n",
    "        if not header: \n",
    "            header = fn[:-3]\n",
    "\n",
    "        cleaned_note = clean(note)\n",
    "        \n",
    "        tokenized_header = tokenizer.encode(header, **encode_kwargs)\n",
    "        tokenized_note = tokenizer.encode(cleaned_note, **encode_kwargs)\n",
    "\n",
    "        note_dict = {'name': fn, 'path':filepath, 'header': header, 'note': cleaned_note, 'tokenized_header':[tokenized_header], 'tokenized_note':[tokenized_note]}\n",
    "\n",
    "        db_df = pd.concat([db_df, pd.DataFrame(note_dict)])\n",
    "    \n",
    "    return db_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df = parse_folder(db_path)\n",
    "# db_df.to_csv('tables/db_df.csv', index = False)\n",
    "# db_df = pd.read_csv('tables/db_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "tokenized_headers = torch.Tensor(np.vstack(db_df.tokenized_header.values)).long()#.to(device)\n",
    "tokenized_notes = torch.Tensor(np.vstack(db_df.tokenized_note.values)).long()#.to(device)\n",
    "\n",
    "# vectorized_headers = model(tokenized_headers)\n",
    "# vectorized_notes = model(tokenized_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_batch(model, tensor, batch_size = 2):\n",
    "    n_chunks = tensor.shape[0] // batch_size\n",
    "    chunked = torch.chunk(tensor, n_chunks)\n",
    "\n",
    "    outputs = []\n",
    "    for batch in chunked:\n",
    "        out = model(batch.to(device)).last_hidden_state[:, 0, :]\n",
    "        outputs.append(out.cpu().detach())\n",
    "    return torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "vectorized_headers = process_by_batch(model, tokenized_headers, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_notes = process_by_batch(model, tokenized_notes, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = db_df.shape[0]\n",
    "# header_embeddings = vectorized_headers.last_hidden_state.reshape(num_notes, -1)\n",
    "# header_embeddings = vectorized_headers.last_hidden_state.mean(dim=-2)\n",
    "header_embeddings = vectorized_headers.last_hidden_state[:, 0, :].detach().numpy()\n",
    "note_embeddings = vectorized_notes.last_hidden_state[:, 0, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=10)\n",
    "cluster = cluster.fit(header_embeddings)\n",
    "header_clusters = cluster.predict(header_embeddings)\n",
    "headers = db_df.header.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.unique(header_clusters):\n",
    "    group = headers[header_clusters == c]\n",
    "    print(f'Cluster {c}\\n{group}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = db_df.note.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=10)\n",
    "cluster = cluster.fit(note_embeddings)\n",
    "note_clusters = cluster.predict(note_embeddings)\n",
    "\n",
    "for c in np.unique(note_clusters):\n",
    "    group = notes[note_clusters == c]\n",
    "    print(f'Cluster {c}\\n{group}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('cudaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13243d513f3ec5f72a4e7429086ed10f2270caad452cf1f965679cad914ac74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
