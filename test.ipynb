{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/home/booydar/Documents/obsidian/fort-knox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, folders, files = next(os.walk(db_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_header(note):\n",
    "    if '## ' in note:\n",
    "        first_header_prefix = '## '\n",
    "        if '### ' in note:\n",
    "            first_header_prefix = '### '\n",
    "\n",
    "        first_header = note.split(first_header_prefix)[1].split('\\n')[0]\n",
    "    else:\n",
    "        first_header = ''\n",
    "\n",
    "    return first_header\n",
    "\n",
    "def clean(note):\n",
    "    # remove zero-links\n",
    "    note = re.sub(r'\\[.*\\]', '', note)\n",
    "\n",
    "    # remove tags and headers\n",
    "    note = re.sub(r'\\#.*\\n', '', note)\n",
    "\n",
    "    # remove \\n\n",
    "    note = re.sub('\\n', ' ', note)\n",
    "\n",
    "    # remove lines\n",
    "    note = re.sub('---', ' ', note)\n",
    "\n",
    "    # remove **\n",
    "    note = re.sub('\\*', '', note)\n",
    "    \n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_thr = 20\n",
    "device = 'cpu'\n",
    "encode_kwargs = {'truncation': True, 'padding': 'max_length', 'pad_to_multiple_of': 1, 'max_length':512}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "db_df = pd.DataFrame()\n",
    "for fn in files:\n",
    "    if '.md' not in fn:\n",
    "        continue\n",
    "    filepath = os.path.join(path, fn)\n",
    "    with open(filepath, 'r') as f:\n",
    "        note = f.read()\n",
    "\n",
    "    if len(note) < length_thr:\n",
    "        continue\n",
    "\n",
    "    header = get_first_header(note)\n",
    "    if not header: \n",
    "        header = fn[:-3]\n",
    "\n",
    "    cleaned_note = clean(note)\n",
    "    \n",
    "    tokenized_header = tokenizer.encode(header, **encode_kwargs)\n",
    "    tokenized_note = tokenizer.encode(cleaned_note, **encode_kwargs)\n",
    "\n",
    "    note_dict = {'name': fn, 'path':filepath, 'header': header, 'note': cleaned_note, 'tokenized_header':[tokenized_header], 'tokenized_note':[tokenized_note]}\n",
    "\n",
    "    db_df = pd.concat([db_df, pd.DataFrame(note_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headers = torch.Tensor(np.vstack(db_df.tokenized_header.values)).long().to(device)\n",
    "tokenized_notes = torch.Tensor(np.vstack(db_df.tokenized_note.values)).long().to(device)\n",
    "# vectorized = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_headers = model(tokenized_headers)\n",
    "# vectorized_notes = model(tokenized_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/booydar/Desktop/_projects/autotagging/test.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/booydar/Desktop/_projects/autotagging/test.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m [\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mstr\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "vectorized_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,2'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(['1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'linux commands.md',\n",
       " 'header': 'linux commands',\n",
       " 'tokenized_header': [101, 12126, 11659, 279, 97386, 10929, 269, 102],\n",
       " 'tokenized_note': [101,\n",
       "  13510,\n",
       "  268,\n",
       "  13776,\n",
       "  130,\n",
       "  196,\n",
       "  130,\n",
       "  242,\n",
       "  130,\n",
       "  254,\n",
       "  10918,\n",
       "  16306,\n",
       "  11270,\n",
       "  10545,\n",
       "  156,\n",
       "  68433,\n",
       "  153,\n",
       "  156,\n",
       "  10918,\n",
       "  16306,\n",
       "  11270,\n",
       "  10545,\n",
       "  156,\n",
       "  68433,\n",
       "  155,\n",
       "  93921,\n",
       "  263,\n",
       "  237,\n",
       "  10710,\n",
       "  10829,\n",
       "  14599,\n",
       "  275,\n",
       "  263,\n",
       "  259,\n",
       "  130,\n",
       "  130,\n",
       "  10685,\n",
       "  74627,\n",
       "  134,\n",
       "  12328,\n",
       "  134,\n",
       "  21018,\n",
       "  47040,\n",
       "  4228,\n",
       "  134,\n",
       "  14322,\n",
       "  65252,\n",
       "  134,\n",
       "  34385,\n",
       "  201,\n",
       "  209,\n",
       "  134,\n",
       "  10616,\n",
       "  17317,\n",
       "  11871,\n",
       "  275,\n",
       "  27023,\n",
       "  134,\n",
       "  274,\n",
       "  263,\n",
       "  259,\n",
       "  132,\n",
       "  10685,\n",
       "  243,\n",
       "  244,\n",
       "  263,\n",
       "  273,\n",
       "  244,\n",
       "  15225,\n",
       "  10725,\n",
       "  271,\n",
       "  130,\n",
       "  248,\n",
       "  138,\n",
       "  130,\n",
       "  236,\n",
       "  263,\n",
       "  273,\n",
       "  201,\n",
       "  12281,\n",
       "  12181,\n",
       "  267,\n",
       "  21185,\n",
       "  73690,\n",
       "  23369,\n",
       "  263,\n",
       "  11180,\n",
       "  269,\n",
       "  10037,\n",
       "  11931,\n",
       "  72291,\n",
       "  12362,\n",
       "  11180,\n",
       "  139,\n",
       "  132,\n",
       "  11180,\n",
       "  141,\n",
       "  13510,\n",
       "  10937,\n",
       "  257,\n",
       "  244,\n",
       "  14335,\n",
       "  19747,\n",
       "  141,\n",
       "  130,\n",
       "  130,\n",
       "  28668,\n",
       "  10829,\n",
       "  9794,\n",
       "  130,\n",
       "  130,\n",
       "  12359,\n",
       "  130,\n",
       "  10685,\n",
       "  74627,\n",
       "  13474,\n",
       "  31024,\n",
       "  10961,\n",
       "  11382,\n",
       "  162,\n",
       "  140,\n",
       "  130,\n",
       "  130,\n",
       "  13849,\n",
       "  16535,\n",
       "  130,\n",
       "  47424,\n",
       "  11822,\n",
       "  241,\n",
       "  286,\n",
       "  242,\n",
       "  243,\n",
       "  19085,\n",
       "  13988,\n",
       "  130,\n",
       "  248,\n",
       "  130,\n",
       "  130,\n",
       "  274,\n",
       "  11324,\n",
       "  10765,\n",
       "  237,\n",
       "  12371,\n",
       "  277,\n",
       "  32706,\n",
       "  16510,\n",
       "  130,\n",
       "  24621,\n",
       "  279,\n",
       "  230,\n",
       "  242,\n",
       "  257,\n",
       "  271,\n",
       "  280,\n",
       "  65821,\n",
       "  39965,\n",
       "  137,\n",
       "  263,\n",
       "  130,\n",
       "  33826,\n",
       "  66570,\n",
       "  269,\n",
       "  136,\n",
       "  130,\n",
       "  242,\n",
       "  274,\n",
       "  145,\n",
       "  255,\n",
       "  141,\n",
       "  130,\n",
       "  268,\n",
       "  156,\n",
       "  274,\n",
       "  30469,\n",
       "  279,\n",
       "  34108,\n",
       "  137,\n",
       "  134,\n",
       "  10616,\n",
       "  275,\n",
       "  134,\n",
       "  12590,\n",
       "  141,\n",
       "  76968,\n",
       "  20551,\n",
       "  156,\n",
       "  42588,\n",
       "  290,\n",
       "  134,\n",
       "  18179,\n",
       "  269,\n",
       "  10710,\n",
       "  10829,\n",
       "  26731,\n",
       "  271,\n",
       "  130,\n",
       "  15841,\n",
       "  12151,\n",
       "  11545,\n",
       "  11124,\n",
       "  274,\n",
       "  145,\n",
       "  255,\n",
       "  141,\n",
       "  63842,\n",
       "  19178,\n",
       "  130,\n",
       "  238,\n",
       "  253,\n",
       "  12899,\n",
       "  42588,\n",
       "  290,\n",
       "  134,\n",
       "  85160,\n",
       "  269,\n",
       "  134,\n",
       "  274,\n",
       "  145,\n",
       "  255,\n",
       "  141,\n",
       "  63842,\n",
       "  19178,\n",
       "  134,\n",
       "  13598,\n",
       "  77336,\n",
       "  11745,\n",
       "  259,\n",
       "  13598,\n",
       "  10710,\n",
       "  10829,\n",
       "  13598,\n",
       "  16010,\n",
       "  10725,\n",
       "  11743,\n",
       "  10710,\n",
       "  10829,\n",
       "  10616,\n",
       "  77996,\n",
       "  9778,\n",
       "  130,\n",
       "  232,\n",
       "  10710,\n",
       "  10829,\n",
       "  16120,\n",
       "  73611,\n",
       "  11556,\n",
       "  274,\n",
       "  145,\n",
       "  255,\n",
       "  141,\n",
       "  63842,\n",
       "  19178,\n",
       "  64218,\n",
       "  59929,\n",
       "  237,\n",
       "  101293,\n",
       "  13142,\n",
       "  278,\n",
       "  15000,\n",
       "  11549,\n",
       "  23042,\n",
       "  130,\n",
       "  130,\n",
       "  12359,\n",
       "  108,\n",
       "  2810,\n",
       "  108,\n",
       "  25359,\n",
       "  209,\n",
       "  261,\n",
       "  199,\n",
       "  9794,\n",
       "  28388,\n",
       "  184,\n",
       "  37301,\n",
       "  177,\n",
       "  130,\n",
       "  136,\n",
       "  102]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-08-22 10:41\n",
      "#nlp  #science #rmt \n",
      "\n",
      "---\n",
      "### Do transformers need long contexts to solve long tasks\n",
      "\n",
      "**Vasily's idea:**\n",
      "Prove that no long attention is needed, instead we can somehow shorten the input so that it fits in the transformer and manages to keep the task quality still nice.\n",
      "\n",
      "### Idea\n",
      "compress long input \n",
      "(i.e. using extractive summarization techniques)\n",
      "and then use a transformer to perform summarization\n",
      "\n",
      "\n",
      "\n",
      "[analog]([https://www.dialog-21.ru/media/5514/iazykovatplusetal037.pdf](https://www.dialog-21.ru/media/5514/iazykovatplusetal037.pdf) attacking russian superGLUE:\n",
      "\n",
      "\n",
      "---\n",
      "[[00 NLP]]\n"
     ]
    }
   ],
   "source": [
    "print(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"12-08-22 10:41Vasily's idea:Prove that no long attention is needed, instead we can somehow shorten the input so that it fits in the transformer and manages to keep the task quality still nice.compress long input (i.e. using extractive summarization techniques)and then use a transformer to perform summarization(https://www.dialog-21.ru/media/5514/iazykovatplusetal037.pdf) attacking russian superGLUE:\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('dpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
