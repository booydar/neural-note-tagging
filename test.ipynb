{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 538/538 [00:00<00:00, 131kB/s]\n",
      "Downloading: 100%|██████████| 409M/409M [01:13<00:00, 5.86MB/s] \n",
      "Some weights of the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 24.0/24.0 [00:00<00:00, 14.9kB/s]\n",
      "Downloading: 100%|██████████| 1.34M/1.34M [00:01<00:00, 1.00MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 27.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "# model_name = 'DeepPavlov/bert-base-multilingual-cased-sentence'\n",
    "# model_name = 'DeepPavlov/distilrubert-small-cased-conversational'\n",
    "model_name = 'DeepPavlov/distilrubert-tiny-cased-conversational'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_header(note):\n",
    "    if '## ' in note:\n",
    "        first_header_prefix = '## '\n",
    "        if '### ' in note:\n",
    "            first_header_prefix = '### '\n",
    "\n",
    "        first_header = note.split(first_header_prefix)[1].split('\\n')[0]\n",
    "    else:\n",
    "        first_header = ''\n",
    "\n",
    "    return first_header\n",
    "\n",
    "def clean(note):\n",
    "    # remove zero-links\n",
    "    note = re.sub(r'\\[.*\\]', '', note)\n",
    "\n",
    "    # remove tags and headers\n",
    "    note = re.sub(r'\\#.*\\n', '', note)\n",
    "\n",
    "    # remove \\n\n",
    "    note = re.sub('\\n', ' ', note)\n",
    "\n",
    "    # remove lines\n",
    "    note = re.sub('---', ' ', note)\n",
    "\n",
    "    # remove **\n",
    "    note = re.sub('\\*', '', note)\n",
    "    \n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_thr = 20\n",
    "device = 'cpu'\n",
    "encode_kwargs = {'truncation': True, 'padding': 'max_length', 'pad_to_multiple_of': 1, 'max_length':512}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/home/booydar/Documents/obsidian/fort-knox'\n",
    "\n",
    "\n",
    "def parse_folder(path):\n",
    "    # print(f'Parsing {path}')\n",
    "\n",
    "    path, folders, files = next(os.walk(path))\n",
    "\n",
    "    db_df = pd.DataFrame()\n",
    "    if len(folders) > 0:\n",
    "        for f in folders:\n",
    "            folder_path = os.path.join(path, f)\n",
    "            f_res_df = parse_folder(folder_path)\n",
    "            db_df = pd.concat([db_df, f_res_df])\n",
    "\n",
    "    for fn in files:\n",
    "        if '.md' not in fn:\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(path, fn)\n",
    "        with open(filepath, 'r') as f:\n",
    "            note = f.read()\n",
    "\n",
    "        if len(note) < length_thr:\n",
    "            continue\n",
    "\n",
    "        header = get_first_header(note)\n",
    "        if not header: \n",
    "            header = fn[:-3]\n",
    "\n",
    "        cleaned_note = clean(note)\n",
    "        \n",
    "        tokenized_header = tokenizer.encode(header, **encode_kwargs)\n",
    "        tokenized_note = tokenizer.encode(cleaned_note, **encode_kwargs)\n",
    "\n",
    "        note_dict = {'name': fn, 'path':filepath, 'header': header, 'note': cleaned_note, 'tokenized_header':[tokenized_header], 'tokenized_note':[tokenized_note]}\n",
    "\n",
    "        db_df = pd.concat([db_df, pd.DataFrame(note_dict)])\n",
    "    \n",
    "    return db_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df = parse_folder(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headers = torch.Tensor(np.vstack(db_df.tokenized_header.values)).long().to(device)\n",
    "tokenized_notes = torch.Tensor(np.vstack(db_df.tokenized_note.values)).long().to(device)\n",
    "\n",
    "vectorized_headers = model(tokenized_headers)\n",
    "vectorized_notes = model(tokenized_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_notes = db_df.shape[0]\n",
    "# header_embeddings = vectorized_headers.last_hidden_state.reshape(num_notes, -1)\n",
    "# header_embeddings = vectorized_headers.last_hidden_state.mean(dim=-2)\n",
    "header_embeddings = vectorized_headers.last_hidden_state[:, 0, :].detach().numpy()\n",
    "note_embeddings = vectorized_notes.last_hidden_state[:, 0, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=10)\n",
    "cluster = cluster.fit(header_embeddings)\n",
    "header_clusters = cluster.predict(header_embeddings)\n",
    "headers = db_df.header.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.unique(header_clusters):\n",
    "    group = headers[header_clusters == c]\n",
    "    print(f'Cluster {c}\\n{group}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = db_df.note.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=10)\n",
    "cluster = cluster.fit(note_embeddings)\n",
    "note_clusters = cluster.predict(note_embeddings)\n",
    "\n",
    "for c in np.unique(note_clusters):\n",
    "    group = notes[note_clusters == c]\n",
    "    print(f'Cluster {c}\\n{group}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('dpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
